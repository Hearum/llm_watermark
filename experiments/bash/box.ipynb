{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fixing the data structure by ensuring proper formatting for the boxplot\n",
    "# Flattening the data into a list of categories, models, and z-scores\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        category_list.extend([category] * 100)\n",
    "        model_list.extend([model] * 100)\n",
    "        zscore_list.extend(data[category][model])\n",
    "\n",
    "# Construct a DataFrame with the corrected format\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# Set up the plot again\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the boxplot with corrected data structure\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=\"Set2\", showfliers=True\n",
    ")\n",
    "\n",
    "# Customizing the plot (axis labels, title, and legend)\n",
    "ax.set_xlabel(\"Dataset Type\")\n",
    "ax.set_ylabel(\"Z-Score\")\n",
    "ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing the dataset so it works correctly with the boxplot\n",
    "# We will use the melted format where each category-model combination corresponds to its z-score.\n",
    "\n",
    "# Creating a proper format by melting the original dictionary into a long-form DataFrame\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['Watermarked', 'Un-watermarked', 'Human']\n",
    "models = [\"OpenGen-GPT2\", \"OpenGen-OPT\", \"OpenGen-LLMa\", \"LFQA-GPT2\", \"LFQA-OPT\", \"LFQA-LLMa\"]\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        category_model.extend([f\"{category}-{model}\"] * 100)\n",
    "        z_scores.extend(data[category][model])\n",
    "\n",
    "# Create the DataFrame again using the correct structure\n",
    "df_long = pd.DataFrame({\n",
    "    'Category-Model': category_model,\n",
    "    'Z-Score': z_scores\n",
    "})\n",
    "\n",
    "# Create the plot now\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot with the corrected data\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category-Model\", y=\"Z-Score\", data=df_long,\n",
    "    palette=\"Set2\", showfliers=True\n",
    ")\n",
    "\n",
    "# Customizing the plot (axis labels, title, and legend)\n",
    "ax.set_xlabel(\"Dataset Type - Model\")\n",
    "ax.set_ylabel(\"Z-Score\")\n",
    "ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Dataset-Methord Visualization Z—score\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['Watermarked', 'Watermarked-attacked', 'Un-watermarked']\n",
    "models = [\"WikiText-LSH\",\"WikiText-KGW\" ,\"C4-LSH\",\"C4-KGW\", \"LFQA-LSH\", \"LFQA-KGW\"]\n",
    "\n",
    "\n",
    "# WikiText_LSH=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# C4_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "# WikiText_KGW=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\" \n",
    "# C4_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"C4_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"LFQA_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/lfqa/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.25_LSH_v2.2_lfqa_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\",\n",
    "    \"C4_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"LFQA_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/lfqa/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_ff-anchored_minhash_prf-4-True-15485863/gen_table_GPT.jsonl_z_score\"\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_z_score\": [], \"w_wm_output_attacked_z_score\": [], \"no_wm_output_z_score\": []}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                \n",
    "                if 'w_wm_output_z_score' in entry:\n",
    "                    data[\"w_wm_output_z_score\"].append(entry[\"w_wm_output_z_score\"])\n",
    "                if 'w_wm_output_attacked_z_score' in entry:\n",
    "                    data[\"w_wm_output_attacked_z_score\"].append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "                if 'no_wm_output_z_score' in entry:\n",
    "                    data[\"no_wm_output_z_score\"].append(entry[\"no_wm_output_z_score\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"Watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_z_score']),\n",
    "    },\n",
    "    \"Watermarked-attacked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "    },\n",
    "    \"Un-watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_z_score']),\n",
    "    }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8),dpi=300)\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('z-score (better →)', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Dataset-Methord Visualization ppl\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['Watermarked', 'Watermarked-attacked', 'Un-watermarked']\n",
    "models = [\"WikiText-LSH\",\"WikiText-KGW\" ,\"C4-LSH\",\"C4-KGW\", \"LFQA-LSH\", \"LFQA-KGW\"]\n",
    "\n",
    "\n",
    "# WikiText_LSH=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# C4_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# LFQA_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "\n",
    "# WikiText_KGW=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_ppl\" \n",
    "# C4_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# LFQA_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"C4_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"LFQA_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/lfqa/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.25_LSH_v2.2_lfqa_new/gen_table_GPT.jsonl_z_score_ppl\",\n",
    "    \"WikiText_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_ppl\",\n",
    "    \"C4_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"LFQA_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_ppl\": [], \"w_wm_output_attacked_ppl\": [], \"no_wm_output_ppl\": []}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                if entry[\"no_wm_output_length\"] >120:\n",
    "                    if 'w_wm_output_ppl' in entry:\n",
    "                        data[\"w_wm_output_ppl\"].append(entry[\"w_wm_output_ppl\"])\n",
    "                    if 'w_wm_output_attacked_ppl' in entry:\n",
    "                        data[\"w_wm_output_attacked_ppl\"].append(entry[\"w_wm_output_attacked_ppl\"])\n",
    "                    if 'no_wm_output_ppl' in entry:\n",
    "                        data[\"no_wm_output_ppl\"].append(entry[\"no_wm_output_ppl\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"Watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_ppl']),\n",
    "    },\n",
    "    \"Watermarked-attacked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "    },\n",
    "    \"Un-watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_ppl']),\n",
    "    }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        zscores = [z for z in zscores if z <= 200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('Text Perplexity (better ←)', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Dataset-Methord Visualization ppl\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['Watermarked', 'Watermarked-attacked', 'Un-watermarked']\n",
    "models = [\"WikiText-LSH\",\"WikiText-KGW\" ,\"C4-LSH\",\"C4-KGW\", \"LFQA-LSH\", \"LFQA-KGW\"]\n",
    "\n",
    "\n",
    "# WikiText_LSH=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# C4_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# LFQA_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "\n",
    "# WikiText_KGW=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_ppl\" \n",
    "# C4_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "# LFQA_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"C4_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"LFQA_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/lfqa/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.25_LSH_v2.2_lfqa_new/gen_table_GPT.jsonl_z_score_ppl\",\n",
    "    \"WikiText_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_ppl\",\n",
    "    \"C4_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\",\n",
    "    \"LFQA_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_ppl\"\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_ppl\": [], \"w_wm_output_attacked_ppl\": [], \"no_wm_output_ppl\": []}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                \n",
    "                if 'w_wm_output_ppl' in entry:\n",
    "                    data[\"w_wm_output_ppl\"].append(entry[\"w_wm_output_ppl\"])\n",
    "                if 'w_wm_output_attacked_ppl' in entry:\n",
    "                    data[\"w_wm_output_attacked_ppl\"].append(entry[\"w_wm_output_attacked_ppl\"])\n",
    "                if 'no_wm_output_ppl' in entry:\n",
    "                    data[\"no_wm_output_ppl\"].append(entry[\"no_wm_output_ppl\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"Watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_ppl']),\n",
    "    },\n",
    "    \"Watermarked-attacked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_attacked_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_attacked_ppl']),\n",
    "    },\n",
    "    \"Un-watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_ppl']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_ppl']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_ppl']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_ppl']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_ppl']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_ppl']),\n",
    "    }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8),dpi=300)\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('Text Perplexity (better ←)', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# PPL Dataset-Methord Visualization\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['Watermarked', 'Watermarked-attacked', 'Un-watermarked']\n",
    "models = [\"WikiText-LSH\", \"C4-LSH\", \"LFQA-LSH\", \"WikiText-KGW\", \"C4-KGW\", \"LFQA-KGW\"]\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"C4_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"LFQA_LSH\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\",\n",
    "    \"C4_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"LFQA_KGW\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_z_score\": [], \"w_wm_output_attacked_z_score\": [], \"no_wm_output_z_score\": []}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                \n",
    "                if 'w_wm_output_z_score' in entry:\n",
    "                    data[\"w_wm_output_z_score\"].append(entry[\"w_wm_output_z_score\"])\n",
    "                if 'w_wm_output_attacked_z_score' in entry:\n",
    "                    data[\"w_wm_output_attacked_z_score\"].append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "                if 'no_wm_output_z_score' in entry:\n",
    "                    data[\"no_wm_output_z_score\"].append(entry[\"no_wm_output_z_score\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"Watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_z_score']),\n",
    "    },\n",
    "    \"Watermarked-attacked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['w_wm_output_attacked_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['w_wm_output_attacked_z_score']),\n",
    "    },\n",
    "    \"Un-watermarked\": {\n",
    "        \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_z_score']),\n",
    "        \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_z_score']),\n",
    "        \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_z_score']),\n",
    "        \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_z_score']),\n",
    "        \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_z_score']),\n",
    "        \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_z_score']),\n",
    "    }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8),dpi=300)\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('Z-Score', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# 测试用的随机生成的模版\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulating z-scores for 5 models under 3 conditions (Watermarked, Un-watermarked, Human)\n",
    "data = {\n",
    "    \"Watermarked\": {\n",
    "        \"OpenGen-GPT2\": np.random.normal(10, 3, 100),\n",
    "        \"OpenGen-OPT\": np.random.normal(11, 2.5, 100),\n",
    "        \"OpenGen-LLMa\": np.random.normal(12, 2, 100),\n",
    "        \"LFQA-GPT2\": np.random.normal(13, 3, 100),\n",
    "        \"LFQA-OPT\": np.random.normal(14, 2.5, 100),\n",
    "        \"LFQA-LLMa\": np.random.normal(15, 1.5, 100),\n",
    "    },\n",
    "    \"Un-watermarked\": {\n",
    "        \"OpenGen-GPT2\": np.random.normal(5, 3, 100),\n",
    "        \"OpenGen-OPT\": np.random.normal(6, 2.5, 100),\n",
    "        \"OpenGen-LLMa\": np.random.normal(7, 2, 100),\n",
    "        \"LFQA-GPT2\": np.random.normal(8, 3, 100),\n",
    "        \"LFQA-OPT\": np.random.normal(9, 2.5, 100),\n",
    "        \"LFQA-LLMa\": np.random.normal(10, 1.5, 100),\n",
    "    },\n",
    "    \"Human\": {\n",
    "        \"OpenGen-GPT2\": np.random.normal(0, 3, 100),\n",
    "        \"OpenGen-OPT\": np.random.normal(1, 2.5, 100),\n",
    "        \"OpenGen-LLMa\": np.random.normal(2, 2, 100),\n",
    "        \"LFQA-GPT2\": np.random.normal(3, 3, 100),\n",
    "        \"LFQA-OPT\": np.random.normal(4, 2.5, 100),\n",
    "        \"LFQA-LLMa\": np.random.normal(5, 1.5, 100),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame format that fits the boxplot\n",
    "data_combined = []\n",
    "categories = list(data.keys())\n",
    "models = list(data[\"Watermarked\"].keys())\n",
    "\n",
    "# Fixing the data structure by ensuring proper formatting for the boxplot\n",
    "# Flattening the data into a list of categories, models, and z-scores\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        category_list.extend([category] * 100)\n",
    "        model_list.extend([model] * 100)\n",
    "        zscore_list.extend(data[category][model])\n",
    "\n",
    "# Construct a DataFrame with the corrected format\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# Set up the plot again\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create the boxplot\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the boxplot with corrected data structure\n",
    "# ax = sns.boxplot(\n",
    "#     x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "#     palette=\"Set2\", showfliers=True\n",
    "# )\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True,linewidth=1.5 \n",
    ")\n",
    "\n",
    "# Customizing the plot (axis labels, title, and legend)\n",
    "# ax.set_xlabel(\"Dataset Type\")\n",
    "ax.set_ylabel(\"Z-Score\")\n",
    "ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import math\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# 加载 z-scores\n",
    "def load_z_scores(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "\n",
    "    # Assuming 'w_wm_output_z_score' and 'no_wm_output_z_score' are the relevant keys\n",
    "    human_z_scores = [entry.get('no_wm_output_z_score', math.nan) for entry in data]\n",
    "    machine_z_scores_unattacked = [entry.get('w_wm_output_z_score', math.nan) for entry in data]\n",
    "    machine_z_scores_01 = [entry.get('w_wm_output_attacked_z_score', math.nan) for entry in data]  # Example for epsilon=0.1\n",
    "    return human_z_scores, machine_z_scores_unattacked, machine_z_scores_01\n",
    "\n",
    "# 清理数据，去除 NaN 值\n",
    "def clean_z_scores(human_z, machine_z):\n",
    "    valid_human_z = [z for z in human_z if not np.isnan(z)]\n",
    "    valid_machine_z = [z for z in machine_z if not np.isnan(z)]\n",
    "    \n",
    "    min_len = min(len(valid_human_z), len(valid_machine_z))\n",
    "    return valid_human_z[:min_len], valid_machine_z[:min_len]\n",
    "\n",
    "# 计算 ROC AUC\n",
    "def calculate_roc_auc(human_z, machine_z):\n",
    "    # assert len(human_z) == len(machine_z)\n",
    "\n",
    "    all_scores = np.concatenate([np.array(human_z), np.array(machine_z)])\n",
    "    \n",
    "    baseline_labels = np.zeros_like(human_z)\n",
    "    watermark_labels = np.ones_like(machine_z)\n",
    "    all_labels = np.concatenate([baseline_labels, watermark_labels])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_scores, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "# 可视化ROC曲线\n",
    "def plot_roc_curve():\n",
    "    ########################################################################################################\n",
    "    # lsh dipper 60\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O60_L60.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    # 绘制 ROC 曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # 绘制不同的曲线\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='lightgreen', label=f'unattacked, AUC:{auc_unattacked:.3f}')\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[3], linewidth=2.5,label=f'LSHash-Dipper(O60,L60), AUC:{auc_01:.3f}')\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='teal', label=f'un, AUC:{auc_unattacked :.3f}')\n",
    "    ########################################################################################################\n",
    "    # lsh dipper 20\n",
    "    data_path =  \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O20_L20.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    # 绘制不同的曲线\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='lightgreen', label=f'unattacked, AUC:{auc_unattacked:.3f}')\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[3], linewidth=2.5,linestyle='-.', label=f'LSHash-Dipper(O20,L20), AUC:{auc_01:.3f}')\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='teal', label=f'un, AUC:{auc_unattacked :.3f}')\n",
    "    ########################################################################################################\n",
    "    # kwg dipper 60\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O60_L60.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color='teal', linewidth=2.5,label=f'SelfHash-Dipper(O60,L60), AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # kwg dipper 20\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O20_L20.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color='teal',linewidth=2.5,linestyle='-.', label=f'SelfHash-Dipper(O20,L20), AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # kwg dipper 20\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_lefthash/gen_table_dipper_O60_L60.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[0],linewidth=2.5, label=f'LeftHash-Dipper(O60,L60), AUC:{auc_01:.3f}')\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # kwg dipper 20\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_lefthash/gen_table_dipper_O20_L20.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[0],linewidth=2.5, linestyle='-.',label=f'LeftHash-Dipper(O20,L20), AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # 绘制基准线\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--', alpha=0.5)\n",
    "    # 设置坐标轴标签和标题\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)  # 设置x轴标签字体大小\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)   # 设置y轴标签字体大小\n",
    "    plt.title('ROC Curve', fontsize=16)              # 设置图表标题字体大小\n",
    "\n",
    "    # 添加图例\n",
    "    plt.legend(loc='lower right', fontsize=11, markerscale=2)  # 放大图例中的标记\n",
    "\n",
    "\n",
    "    # 显示图形\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 调用可视化函数\n",
    "plot_roc_curve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import math\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "def load_z_scores(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "\n",
    "    # Assuming 'w_wm_output_z_score' and 'no_wm_output_z_score' are the relevant keys\n",
    "    human_z_scores = [entry.get('no_wm_output_z_score', math.nan) for entry in data]\n",
    "    machine_z_scores_unattacked = [entry.get('w_wm_output_z_score', math.nan) for entry in data]\n",
    "    machine_z_scores_01 = [entry.get('w_wm_output_attacked_z_score', math.nan) for entry in data]  # Example for epsilon=0.1\n",
    "    return human_z_scores, machine_z_scores_unattacked, machine_z_scores_01\n",
    "\n",
    "def clean_z_scores(human_z, machine_z):\n",
    "    valid_human_z = [z for z in human_z if not np.isnan(z)]\n",
    "    valid_machine_z = [z for z in machine_z if not np.isnan(z)]\n",
    "    \n",
    "    min_len = min(len(valid_human_z), len(valid_machine_z))\n",
    "    return valid_human_z[:min_len], valid_machine_z[:min_len]\n",
    "\n",
    "def calculate_roc_auc(human_z, machine_z):\n",
    "    # assert len(human_z) == len(machine_z)\n",
    "\n",
    "    all_scores = np.concatenate([np.array(human_z), np.array(machine_z)])\n",
    "    \n",
    "    baseline_labels = np.zeros_like(human_z)\n",
    "    watermark_labels = np.ones_like(machine_z)\n",
    "    all_labels = np.concatenate([baseline_labels, watermark_labels])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_scores, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "def plot_roc_curve():\n",
    "    ########################################################################################################\n",
    "    # lsh dipper 60\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    # 绘制 ROC 曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # 绘制不同的曲线\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='lightgreen', label=f'unattacked, AUC:{auc_unattacked:.3f}')\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[3], linewidth=2.5,label=f'LSHash-GPT Attack, AUC:{auc_01:.3f}')\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='teal', label=f'un, AUC:{auc_unattacked :.3f}')\n",
    "    ########################################################################################################\n",
    "    # lsh dipper 20\n",
    "    data_path =  \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_deepseek_attacker.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    # 绘制不同的曲线\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='lightgreen', label=f'unattacked, AUC:{auc_unattacked:.3f}')\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[3], linewidth=2.5,linestyle='-.', label=f'LSHash-Deepseek Attack, AUC:{auc_01:.3f}')\n",
    "    # plt.plot(fpr_unattacked, tpr_unattacked, color='teal', label=f'un, AUC:{auc_unattacked :.3f}')\n",
    "    ########################################################################################################\n",
    "    # kwg dipper 60\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color='teal', linewidth=2.5,label=f'SelfHash-GPT Attack, AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # kwg dipper 20\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_deepseek_attacker.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color='teal',linewidth=2.5,linestyle='-.', label=f'SelfHash-Deepseek Attack, AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # add gpt\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_addhash_wikit/gen_table_GPT.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[2],linewidth=2.5,linestyle='-', label=f'AddfHash-GPT Attack, AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # add ds\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_addhash_wikit/gen_table_deepseek_attacker.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[2],linewidth=2.5,linestyle='-.', label=f'AddfHash-DeepSeek Attack, AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "\n",
    "    # left gpt\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_lefthash/gen_table_GPT.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[0],linewidth=2.5,linestyle='-', label=f'LeftHash-GPT Attack, AUC:{auc_01:.3f}')\n",
    "    ########################################################################################################\n",
    "    # left ds\n",
    "    data_path = \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_lefthash/gen_table_deepseek_attacker.jsonl_z_score\"\n",
    "    \n",
    "    human_z, machine_z_unattacked, machine_z_01 = load_z_scores(data_path)\n",
    "\n",
    "    human_z, machine_z_unattacked = clean_z_scores(human_z, machine_z_unattacked)\n",
    "    human_z, machine_z_01 = clean_z_scores(human_z, machine_z_01)\n",
    "\n",
    "    # dipper attack数据曲线\n",
    "    fpr_unattacked, tpr_unattacked, auc_unattacked = calculate_roc_auc(human_z, machine_z_unattacked)\n",
    "    # unattacked数据曲线\n",
    "    fpr_01, tpr_01, auc_01 = calculate_roc_auc(human_z, machine_z_01)\n",
    "\n",
    "    plt.plot(fpr_01, tpr_01, color=colors[0],linewidth=2.5,linestyle='-.', label=f'LeftfHash-DeepSeek Attack, AUC:{auc_01:.3f}')\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # 绘制基准线\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--', alpha=0.5)\n",
    "    # 设置坐标轴标签和标题\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)  # 设置x轴标签字体大小\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)   # 设置y轴标签字体大小\n",
    "    plt.title('ROC Curve', fontsize=16)              # 设置图表标题字体大小\n",
    "\n",
    "    # 添加图例\n",
    "    plt.legend(loc='lower right', fontsize=11, markerscale=2)  # 放大图例中的标记\n",
    "\n",
    "\n",
    "    # 显示图形\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 调用可视化函数\n",
    "plot_roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Dataset-Methord Visualization\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['WikiText-LSH', 'WikiText-SelfHash']\n",
    "models = [\"Wateramrked\",\n",
    "        \"Dipper(O60,L60)\",\n",
    "        \"Dipper(O20,L20)\",\n",
    "        \"Deepseek Attacked\",\n",
    "        \"GPT Attacked\",\n",
    "        \"Unwatermarked\"]\n",
    "\n",
    "# WikiText_LSH=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# C4_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "# WikiText_KGW=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\" \n",
    "# C4_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH_GPT\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_LSH_dipper60\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O60_L60.jsonl_z_score\",\n",
    "    \"WikiText_LSH_dipper20\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O20_L20.jsonl_z_score\",\n",
    "    \"WikiText_LSH_deepseek\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_deepseek_attacker.jsonl_z_score\",\n",
    "    \"WikiText_KGW_GPT\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_KGW_dipper60\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O60_L60.jsonl_z_score\",\n",
    "    \"WikiText_KGW_dipper20\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O20_L20.jsonl_z_score\",\n",
    "    \"WikiText_KGW_deepseek\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_deepseek_attacker.jsonl_z_score\",\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_z_score\": [], \"w_wm_output_attacked_z_score\": [], \"no_wm_output_z_score\": []}\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                \n",
    "                if 'w_wm_output_z_score' in entry:\n",
    "                    data[\"w_wm_output_z_score\"].append(entry[\"w_wm_output_z_score\"])\n",
    "                if 'w_wm_output_attacked_z_score' in entry:\n",
    "                    data[\"w_wm_output_attacked_z_score\"].append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "                if 'no_wm_output_z_score' in entry:\n",
    "                    data[\"no_wm_output_z_score\"].append(entry[\"no_wm_output_z_score\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"WikiText-LSH\": {\n",
    "        \"Wateramrked\": np.array(all_data[\"WikiText_LSH_GPT\"]['w_wm_output_z_score']),\n",
    "        \"Dipper(O60,L60)\": np.array(all_data[\"WikiText_LSH_dipper60\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Dipper(O20,L20)\": np.array(all_data[\"WikiText_LSH_dipper20\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Deepseek Attacked\": np.array(all_data[\"WikiText_LSH_deepseek\"]['w_wm_output_attacked_z_score']),\n",
    "        \"GPT Attacked\": np.array(all_data[\"WikiText_LSH_GPT\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Unwatermarked\": np.array(all_data[\"WikiText_LSH_GPT\"]['no_wm_output_z_score']),\n",
    "    },\n",
    "    \"WikiText-SelfHash\": {\n",
    "        \"Wateramrked\": np.array(all_data[\"WikiText_KGW_GPT\"]['w_wm_output_z_score']),\n",
    "        \"Dipper(O60,L60)\": np.array(all_data[\"WikiText_KGW_dipper60\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Dipper(O20,L20)\": np.array(all_data[\"WikiText_KGW_dipper20\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Deepseek Attacked\": np.array(all_data[\"WikiText_KGW_deepseek\"]['w_wm_output_attacked_z_score']),\n",
    "        \"GPT Attacked\": np.array(all_data[\"WikiText_KGW_GPT\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Unwatermarked\": np.array(all_data[\"WikiText_KGW_GPT\"]['no_wm_output_z_score']),\n",
    "    },\n",
    "    # \"Un-watermarked\": {\n",
    "    #     \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_z_score']),\n",
    "    #     \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_z_score']),\n",
    "    #     \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_z_score']),\n",
    "    # }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('z-score ', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Dataset-Methord Visualization\n",
    "################################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# Generating synthetic data for the boxplot (since actual data isn't provided)\n",
    "np.random.seed(42)\n",
    "\n",
    "category_model = []\n",
    "z_scores = []\n",
    "categories = ['WikiText-LSH', 'WikiText-SelfHash']\n",
    "models = [\"Wateramrked\",\n",
    "        \"Dipper(O60,L60)\",\n",
    "        \"Dipper(O20,L20)\",\n",
    "        \"Deepseek Attacked\",\n",
    "        \"GPT Attacked\",\n",
    "        \"Unwatermarked\"]\n",
    "\n",
    "# WikiText_LSH=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# C4_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_LSH_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "# WikiText_KGW=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\" \n",
    "# C4_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "# LFQA_KGW_path=\"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/delta2_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_self_wiki_c4_new/gen_table_GPT.jsonl_z_score\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 路径列表\n",
    "paths = {\n",
    "    \"WikiText_LSH_GPT\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_LSH_dipper60\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O60_L60.jsonl_z_score\",\n",
    "    \"WikiText_LSH_dipper20\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_dipper_O20_L20.jsonl_z_score\",\n",
    "    \"WikiText_LSH_deepseek\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_deepseek_attacker.jsonl_z_score\",\n",
    "    \"WikiText_KGW_GPT\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score\",\n",
    "    \"WikiText_KGW_dipper60\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O60_L60.jsonl_z_score\",\n",
    "    \"WikiText_KGW_dipper20\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_dipper_O20_L20.jsonl_z_score\",\n",
    "    \"WikiText_KGW_deepseek\": \"/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_deepseek_attacker.jsonl_z_score\",\n",
    "}\n",
    "\n",
    "# 用于存储数据的字典\n",
    "all_data = {}\n",
    "\n",
    "# 处理每个路径\n",
    "for key, path in paths.items():\n",
    "    data = {\"w_wm_output_z_score\": [], \"w_wm_output_attacked_z_score\": [], \"no_wm_output_z_score\": []}\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                \n",
    "                if 'w_wm_output_z_score' in entry:\n",
    "                    data[\"w_wm_output_z_score\"].append(entry[\"w_wm_output_z_score\"])\n",
    "                if 'w_wm_output_attacked_z_score' in entry:\n",
    "                    data[\"w_wm_output_attacked_z_score\"].append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "                if 'no_wm_output_z_score' in entry:\n",
    "                    data[\"no_wm_output_z_score\"].append(entry[\"no_wm_output_z_score\"])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "    \n",
    "    all_data[key] = data\n",
    "\n",
    "# 模拟 Z-Score 数据\n",
    "data = {\n",
    "    \"Wateramrked\": {\n",
    "        \"Wateramrked\": np.array(all_data[\"WikiText_LSH_GPT\"]['w_wm_output_z_score']),\n",
    "        \"Dipper(O60,L60)\": np.array(all_data[\"WikiText_LSH_dipper60\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Dipper(O20,L20)\": np.array(all_data[\"WikiText_LSH_dipper20\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Deepseek Attacked\": np.array(all_data[\"WikiText_LSH_deepseek\"]['w_wm_output_attacked_z_score']),\n",
    "        \"GPT Attacked\": np.array(all_data[\"WikiText_LSH_GPT\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Unwatermarked\": np.array(all_data[\"WikiText_LSH_GPT\"]['no_wm_output_z_score']),\n",
    "    },\n",
    "    \"WikiText-SelfHash\": {\n",
    "        \"Wateramrked\": np.array(all_data[\"WikiText_KGW_GPT\"]['w_wm_output_z_score']),\n",
    "        \"Dipper(O60,L60)\": np.array(all_data[\"WikiText_KGW_dipper60\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Dipper(O20,L20)\": np.array(all_data[\"WikiText_KGW_dipper20\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Deepseek Attacked\": np.array(all_data[\"WikiText_KGW_deepseek\"]['w_wm_output_attacked_z_score']),\n",
    "        \"GPT Attacked\": np.array(all_data[\"WikiText_KGW_GPT\"]['w_wm_output_attacked_z_score']),\n",
    "        \"Unwatermarked\": np.array(all_data[\"WikiText_KGW_GPT\"]['no_wm_output_z_score']),\n",
    "    },\n",
    "    # \"Un-watermarked\": {\n",
    "    #     \"WikiText-LSH\": np.array(all_data[\"WikiText_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"C4-LSH\": np.array(all_data[\"C4_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"LFQA-LSH\": np.array(all_data[\"LFQA_LSH\"]['no_wm_output_z_score']),\n",
    "    #     \"WikiText-KGW\": np.array(all_data[\"WikiText_KGW\"]['no_wm_output_z_score']),\n",
    "    #     \"C4-KGW\": np.array(all_data[\"C4_KGW\"]['no_wm_output_z_score']),\n",
    "    #     \"LFQA-KGW\": np.array(all_data[\"LFQA_KGW\"]['no_wm_output_z_score']),\n",
    "    # }\n",
    "}\n",
    "\n",
    "category_list = []\n",
    "model_list = []\n",
    "zscore_list = []\n",
    "\n",
    "for category in categories:\n",
    "    for model in models:\n",
    "        # Check that the z-scores have the same length\n",
    "        zscores = data[category][model][:200]\n",
    "        # Extend the lists based on the actual data length\n",
    "        category_list.extend([category] * len(zscores))\n",
    "        model_list.extend([model] * len(zscores))\n",
    "        zscore_list.extend(zscores)\n",
    "\n",
    "df_corrected = pd.DataFrame({\n",
    "    \"Category\": category_list,\n",
    "    \"Model\": model_list,\n",
    "    \"Z-Score\": zscore_list\n",
    "})\n",
    "\n",
    "# 设置绘图\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 创建箱型图\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 定制离群点的样式为菱形\n",
    "flierprops = dict(marker='D', markersize=6, alpha=0.7)\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"Category\", y=\"Z-Score\", hue=\"Model\", data=df_corrected,\n",
    "    palette=colors, showfliers=True, linewidth=2.0)\n",
    "\n",
    "plt.legend(title=\"Dataset-Model\", loc=\"upper right\", bbox_to_anchor=(1, 0))\n",
    "# 定制化图表\n",
    "ax.set_ylabel('Z-Score', fontsize=14)  # 增加字体大小\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels(['Watermarked', 'Watermarked-attacked', 'Un-watermarked'], fontsize=14)\n",
    "# ax.set_title(\"Boxplot of Z-Scores by Dataset Type and Model\")\n",
    "plt.legend(title=\"Dataset-Method\", bbox_to_anchor=(0.78, 0.95), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "# N = 500\n",
    "# 初始化数据存储\n",
    "ppl_data = []\n",
    "z_score_data = []\n",
    "length_data = []\n",
    "psp = []\n",
    "# 读取文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data.append(entry['w_wm_output_length'])\n",
    "            psp.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "# ppl_data = ppl_data[:N]\n",
    "# z_score_data = z_score_data[:N]\n",
    "# length_data = length_data[:N]\n",
    "# psp = psp[:N]\n",
    "\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "length_min = np.min(length_data)\n",
    "length_max = np.max(length_data)\n",
    "normalized_length = (np.array(length_data) - length_min) / (length_max - length_min)  # 归一化到0到1之间\n",
    "size = (normalized_length * 100)  # 调整大小，使其更适合散点图\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(ppl_data, z_score_data, s=size, c=psp, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL (better ←)', fontsize=12)\n",
    "ax.set_ylabel('Z-Score (better →)', fontsize=12)\n",
    "# ax.set_title('Scatter plot of PPL vs Z-Score with Length-based Marker Size', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "\n",
    "# 初始化数据存储\n",
    "ppl_data = []\n",
    "z_score_data = []\n",
    "length_data = []\n",
    "psp = []\n",
    "# 读取文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data.append(entry['w_wm_output_length'])\n",
    "            psp.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "# ppl_data = ppl_data[:N]\n",
    "# z_score_data = z_score_data[:N]\n",
    "# length_data = length_data[:N]\n",
    "# psp = psp[:N]\n",
    "\n",
    "\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "length_min = np.min(length_data)\n",
    "length_max = np.max(length_data)\n",
    "normalized_length = (np.array(length_data) - length_min) / (length_max - length_min)  # 归一化到0到1之间\n",
    "size = (normalized_length * 100)  # 调整大小，使其更适合散点图\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(ppl_data, z_score_data, s=size, c=psp, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL (better ←)', fontsize=12)\n",
    "ax.set_ylabel('Z-Score (better →)', fontsize=12)\n",
    "# ax.set_title('Scatter plot of PPL vs Z-Score with Length-based Marker Size', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "\n",
    "# 初始化数据存储\n",
    "ppl_data = []\n",
    "z_score_data = []\n",
    "length_data = []\n",
    "psp = []\n",
    "\n",
    "# 读取文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data.append(entry['w_wm_output_length'])\n",
    "            psp.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "# ppl_data = ppl_data[:N]\n",
    "# z_score_data = z_score_data[:N]\n",
    "# length_data = length_data[:N]\n",
    "# psp = psp[:N]\n",
    "\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "length_min = np.min(length_data)\n",
    "length_max = np.max(length_data)\n",
    "normalized_length = (np.array(length_data) - length_min) / (length_max - length_min)  # 归一化到0到1之间\n",
    "size = (normalized_length * 100)  # 调整大小，使其更适合散点图\n",
    "\n",
    "# 计算重心 (均值坐标)\n",
    "center_x = np.mean(ppl_data)\n",
    "center_y = np.mean(z_score_data)\n",
    "print(center_x,center_y)\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(ppl_data, z_score_data, c=psp, cmap='viridis', alpha=0.6, s=size)\n",
    "\n",
    "# 添加重心标记\n",
    "ax.scatter(center_x, center_y,color='#B55D60', marker='*', s=200)\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL', fontsize=12)\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 显示重心图例\n",
    "ax.legend()\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "# N = 500\n",
    "# 初始化数据存储\n",
    "ppl_data = []\n",
    "z_score_data = []\n",
    "length_data = []\n",
    "psp = []\n",
    "# 读取文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data.append(entry['w_wm_output_length'])\n",
    "            psp.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "# ppl_data = ppl_data[:N]\n",
    "# z_score_data = z_score_data[:N]\n",
    "# length_data = length_data[:N]\n",
    "# psp = psp[:N]\n",
    "\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "length_min = np.min(length_data)\n",
    "length_max = np.max(length_data)\n",
    "normalized_length = (np.array(length_data) - length_min) / (length_max - length_min)  # 归一化到0到1之间\n",
    "size = (normalized_length * 100)  # 调整大小，使其更适合散点图\n",
    "\n",
    "\n",
    "# 计算重心 (均值坐标)\n",
    "center_x = np.mean(ppl_data)\n",
    "center_y = np.mean(z_score_data)\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(ppl_data, z_score_data, c=psp, cmap='viridis', alpha=0.6, s=size)\n",
    "\n",
    "# 添加重心标记\n",
    "ax.scatter(center_x, center_y,color='#B55D60', marker='*', s=200)\n",
    "print(center_x,center_y)\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL', fontsize=12)\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "# ax.set_title('Scatter plot of PPL vs Z-Score with Length-based Marker Size', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "N=500\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "path2 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "# 初始化数据存储\n",
    "ppl_data_1 = []\n",
    "z_score_data_1 = []\n",
    "length_data_1 = []\n",
    "psp_1 = []\n",
    "\n",
    "ppl_data_2 = []\n",
    "z_score_data_2 = []\n",
    "length_data_2 = []\n",
    "psp_2 = []\n",
    "\n",
    "# 读取第一个文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_1.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data_1.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data_1.append(entry['w_wm_output_length'])\n",
    "            psp_1.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "\n",
    "# 读取第二个文件并解析数据\n",
    "with open(path2, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_2.append(entry[\"w_wm_output_attacked_ppl\"])\n",
    "            z_score_data_2.append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "            length_data_2.append(entry['w_wm_output_attacked_length'])\n",
    "            \n",
    "            # psp_2.append(entry['wm_and_no_wm_psp'])\n",
    "        except (json.JSONDecodeError,KeyError) as e:\n",
    "            pass\n",
    "\n",
    "ppl_data_1 = ppl_data_1[:N]\n",
    "z_score_data_1 = z_score_data_1[:N]\n",
    "length_data_1 = length_data_1[:N]\n",
    "psp_1 = psp_1[:N]\n",
    "\n",
    "ppl_data_2 = ppl_data_2[:N]\n",
    "z_score_data_2 = z_score_data_2[:N]\n",
    "length_data_2 = length_data_2[:N]\n",
    "psp2 = psp_1\n",
    "psp_2 = psp_2[:N]\n",
    "print(len(ppl_data_2))\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "def normalize_length(length_data):\n",
    "    length_min = np.min(length_data)\n",
    "    length_max = np.max(length_data)\n",
    "    return (np.array(length_data) - length_min) / (length_max - length_min)\n",
    "\n",
    "normalized_length_1 = normalize_length(length_data_1)\n",
    "normalized_length_2 = normalize_length(length_data_2)\n",
    "\n",
    "size_1 = (normalized_length_1 * 100)  # 调整大小，使其更适合散点图\n",
    "size_2 = (normalized_length_2 * 100)\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 第一组数据（path1）\n",
    "scatter1 = ax.scatter(ppl_data_1, z_score_data_1, s=size_1, c=psp_1, cmap='viridis', alpha=0.6, marker='o', label='Path 1')\n",
    "\n",
    "# 第二组数据（path2），使用不同的散点形状\n",
    "scatter2 = ax.scatter(ppl_data_2, z_score_data_2,s=size_2, cmap='plasma', alpha=0.6, marker='^', label='Path 2')\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL', fontsize=12)\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "# ax.set_title('Scatter plot of PPL vs Z-Score for Two Paths', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar1 = plt.colorbar(scatter1)\n",
    "cbar1.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# cbar2 = plt.colorbar(scatter2)\n",
    "# cbar2.set_label('Normalized Length (Path 2)', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend()\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "N=500\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "path2 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "# 初始化数据存储\n",
    "ppl_data_1 = []\n",
    "z_score_data_1 = []\n",
    "length_data_1 = []\n",
    "psp_1 = []\n",
    "\n",
    "ppl_data_2 = []\n",
    "z_score_data_2 = []\n",
    "length_data_2 = []\n",
    "psp_2 = []\n",
    "\n",
    "# 读取第一个文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_1.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data_1.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data_1.append(entry['w_wm_output_length'])\n",
    "            psp_1.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "\n",
    "# 读取第二个文件并解析数据\n",
    "with open(path2, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_2.append(entry[\"w_wm_output_attacked_ppl\"])\n",
    "            z_score_data_2.append(entry[\"w_wm_output_attacked_z_score\"])\n",
    "            length_data_2.append(entry['w_wm_output_attacked_length'])\n",
    "            \n",
    "            # psp_2.append(entry['wm_and_no_wm_psp'])\n",
    "        except (json.JSONDecodeError,KeyError) as e:\n",
    "            pass\n",
    "\n",
    "ppl_data_1 = ppl_data_1[:N]\n",
    "z_score_data_1 = z_score_data_1[:N]\n",
    "length_data_1 = length_data_1[:N]\n",
    "psp_1 = psp_1[:N]\n",
    "\n",
    "ppl_data_2 = ppl_data_2[:N]\n",
    "z_score_data_2 = z_score_data_2[:N]\n",
    "length_data_2 = length_data_2[:N]\n",
    "psp2 = psp_1\n",
    "psp_2 = psp_2[:N]\n",
    "print(len(ppl_data_2))\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "def normalize_length(length_data):\n",
    "    length_min = np.min(length_data)\n",
    "    length_max = np.max(length_data)\n",
    "    return (np.array(length_data) - length_min) / (length_max - length_min)\n",
    "\n",
    "normalized_length_1 = normalize_length(length_data_1)\n",
    "normalized_length_2 = normalize_length(length_data_2)\n",
    "\n",
    "size_1 = (normalized_length_1 * 100)  # 调整大小，使其更适合散点图\n",
    "size_2 = (normalized_length_2 * 100)\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 第一组数据（path1）\n",
    "scatter1 = ax.scatter(ppl_data_1, z_score_data_1, s=size_1, c=psp_1, cmap='viridis', alpha=0.6, marker='o', label='Path 1')\n",
    "\n",
    "# 第二组数据（path2），使用不同的散点形状\n",
    "scatter2 = ax.scatter(ppl_data_2, z_score_data_2,s=size_2, cmap='plasma', alpha=0.6, marker='^', label='Path 2')\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL', fontsize=12)\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "# ax.set_title('Scatter plot of PPL vs Z-Score for Two Paths', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar1 = plt.colorbar(scatter1)\n",
    "cbar1.set_label('P-SP', fontsize=12)\n",
    "\n",
    "# cbar2 = plt.colorbar(scatter2)\n",
    "# cbar2.set_label('Normalized Length (Path 2)', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend()\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "N=350\n",
    "\n",
    "# 数据路径\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'\n",
    "path2 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_KWG_width_4_selfhash_wikit/gen_table_GPT.jsonl_z_score_ppl_psp'  # 替换为实际的第二个数据文件路径\n",
    "\n",
    "# 初始化数据存储\n",
    "ppl_data_1 = []\n",
    "z_score_data_1 = []\n",
    "length_data_1 = []\n",
    "psp_1 = []\n",
    "\n",
    "ppl_data_2 = []\n",
    "z_score_data_2 = []\n",
    "length_data_2 = []\n",
    "psp_2 = []\n",
    "\n",
    "# 读取第一个文件并解析数据\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_1.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data_1.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data_1.append(entry['w_wm_output_length'])\n",
    "            psp_1.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "\n",
    "# 读取第二个文件并解析数据\n",
    "with open(path2, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            ppl_data_2.append(entry[\"w_wm_output_ppl\"])\n",
    "            z_score_data_2.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data_2.append(entry['w_wm_output_length'])\n",
    "            psp_2.append(entry['wm_and_no_wm_psp'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "ppl_data_1 = ppl_data_1[:N]\n",
    "z_score_data_1 = z_score_data_1[:N]\n",
    "length_data_1 = length_data_1[:N]\n",
    "psp_1 = psp_1[:N]\n",
    "\n",
    "ppl_data_2 = ppl_data_2[:N]\n",
    "z_score_data_2 = z_score_data_2[:N]\n",
    "length_data_2 = length_data_2[:N]\n",
    "psp2 = psp_1\n",
    "psp_2 = psp_2[:N]\n",
    "\n",
    "# 最小-最大归一化处理长度数据，以决定点的大小\n",
    "def normalize_length(length_data):\n",
    "    length_min = np.min(length_data)\n",
    "    length_max = np.max(length_data)\n",
    "    return (np.array(length_data) - length_min) / (length_max - length_min)\n",
    "\n",
    "normalized_length_1 = normalize_length(length_data_1)\n",
    "normalized_length_2 = normalize_length(length_data_2)\n",
    "\n",
    "size_1 = (normalized_length_1 * 100)  # 调整大小，使其更适合散点图\n",
    "size_2 = (normalized_length_2 * 100)\n",
    "\n",
    "# 绘制散点图\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 第一组数据（path1）\n",
    "scatter1 = ax.scatter(ppl_data_1, z_score_data_1, s=size_1, c=psp_1, cmap='viridis', alpha=0.6, marker='o', label='Path 1')\n",
    "\n",
    "# 第二组数据（path2），使用不同的散点形状\n",
    "scatter2 = ax.scatter(ppl_data_2, z_score_data_2, s=size_2, c=psp_2, cmap='viridis', alpha=0.6, marker='^', label='Path 2')\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel('PPL', fontsize=15)\n",
    "ax.set_ylabel('Z-Score', fontsize=15)\n",
    "ax.set_title('Scatter plot of PPL vs Z-Score for Two Paths', fontsize=16)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar1 = plt.colorbar(scatter1)\n",
    "cbar1.set_label('Normalized Length (Path 1)', fontsize=12)\n",
    "\n",
    "# cbar2 = plt.colorbar(scatter2)\n",
    "# cbar2.set_label('Normalized Length (Path 2)', fontsize=12)\n",
    "\n",
    "# 显示网格\n",
    "ax.grid(True)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend()\n",
    "\n",
    "# 自动调整布局\n",
    "fig.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded the data as follows\n",
    "# Initialize data storage\n",
    "ppl_data_1 = []\n",
    "z_score_data_1 = []\n",
    "length_data_1 = []\n",
    "psp_1 = []\n",
    "\n",
    "# Path to the data file\n",
    "path1 = '/home/shenhm/documents/lm-watermarking/watermark_reliability_release/output/wikitext/delta5_len_150/llama_7B_N500_T200_no_filter_batch_1_delta_5_gamma_0.25_LshParm_6_32_0.2_LSH_v2.2_c4_new/gen_table_GPT.jsonl_z_score_ppl_psp'  # Replace with the actual path to your file\n",
    "\n",
    "# Read and parse the first file\n",
    "with open(path1, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            z_score_data_1.append(entry[\"w_wm_output_z_score\"])\n",
    "            length_data_1.append(entry['w_wm_output_length'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析失败: {e}, 跳过这一行\")\n",
    "\n",
    "# Convert data to numpy arrays for easy manipulation\n",
    "length_data_1 = np.array(length_data_1)\n",
    "z_score_data_1 = np.array(z_score_data_1)\n",
    "\n",
    "data = pd.DataFrame({'length': length_data_1, 'z_score': z_score_data_1})\n",
    "\n",
    "# 数据预处理：按length分组计算均值和标准差\n",
    "# 使用pd.qcut自动分组（可根据需要改用pd.cut）\n",
    "data_sorted = data.sort_values('length')\n",
    "grouped = data_sorted.groupby('length', as_index=False).agg(\n",
    "    mean_z=('z_score', 'mean'),\n",
    "    std_z=('z_score', 'std')\n",
    ")\n",
    "\n",
    "# 处理可能存在的NaN值（当某个length组只有一个样本时）\n",
    "grouped['std_z'].fillna(0, inplace=True)\n",
    "\n",
    "# 可视化设置\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grouped['length'], \n",
    "         grouped['mean_z'], \n",
    "         label='Z-Score Trend', \n",
    "         color='royalblue',\n",
    "         linewidth=2)\n",
    "\n",
    "plt.fill_between(grouped['length'],\n",
    "                 grouped['mean_z'] - grouped['std_z'],\n",
    "                 grouped['mean_z'] + grouped['std_z'],\n",
    "                 color='lightblue',\n",
    "                 alpha=0.4,\n",
    "                 label='±1 Std. Dev.')\n",
    "\n",
    "# 图表装饰\n",
    "plt.xlabel('Length', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Z-Score', fontsize=12, labelpad=10)\n",
    "plt.title('Z-Score Variation with Length', fontsize=14, pad=20)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 示例数据生成（替换为真实数据）\n",
    "np.random.seed(42)\n",
    "lengths = np.random.randint(10, 100, 200)\n",
    "z_scores = 0.5 * lengths + np.random.normal(0, 5, 200)\n",
    "data = pd.DataFrame({'length': lengths, 'z_score': z_scores})\n",
    "\n",
    "# 数据预处理：按length分组计算均值和标准差\n",
    "# 使用pd.qcut自动分组（可根据需要改用pd.cut）\n",
    "data_sorted = data.sort_values('length')\n",
    "grouped = data_sorted.groupby('length', as_index=False).agg(\n",
    "    mean_z=('z_score', 'mean'),\n",
    "    std_z=('z_score', 'std')\n",
    ")\n",
    "\n",
    "# 处理可能存在的NaN值（当某个length组只有一个样本时）\n",
    "grouped['std_z'].fillna(0, inplace=True)\n",
    "\n",
    "# 可视化设置\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grouped['length'], \n",
    "         grouped['mean_z'], \n",
    "         label='Z-Score Trend', \n",
    "         color='royalblue',\n",
    "         linewidth=2)\n",
    "\n",
    "plt.fill_between(grouped['length'],\n",
    "                 grouped['mean_z'] - grouped['std_z'],\n",
    "                 grouped['mean_z'] + grouped['std_z'],\n",
    "                 color='lightblue',\n",
    "                 alpha=0.4,\n",
    "                 label='±1 Std. Dev.')\n",
    "\n",
    "# 图表装饰\n",
    "plt.xlabel('Length', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Z-Score', fontsize=12, labelpad=10)\n",
    "plt.title('Z-Score Variation with Length', fontsize=14, pad=20)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '$h = 2$'),\n",
       " Text(1, 0, '$h = 3$'),\n",
       " Text(2, 0, '$h = 4$'),\n",
       " Text(3, 0, '$h = 5$'),\n",
       " Text(4, 0, '$h = 6$'),\n",
       " Text(5, 0, '$h = 8$')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "\n",
    "y_data = [ 0.95, 0.91, 0.86, 0.32, 0.06,0.03]\n",
    "\n",
    "std_devs = [0.02, 0.023, 0.018, 0.02]\n",
    "\n",
    "x_labels = [r'$h = 2$', r'$h = 3$',r'$h = 4$',r'$h = 5$',r'$h = 6$',r'$h = 8$']\n",
    "x_data = np.arange(len(x_labels))\n",
    "\n",
    "\n",
    "\n",
    "yerr_top = [y_data + std_devs for y_data, std_devs in zip(y_data, std_devs)]\n",
    "yerr_bot = [y_data - std_devs for y_data, std_devs in zip(y_data, std_devs)]\n",
    "\n",
    "\n",
    "# 柱状图颜色\n",
    "colors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "edgecolors = ['#5975A4','#CC8963','#5F9E6E','#B55D60','#857AAB','#8D7866']\n",
    "\n",
    "# 柱状图\n",
    "bar = plt.bar(x_data, y_data,color=colors,edgecolor='black', linewidth=1.5,width=0.5)\n",
    "\n",
    "# plt.grid(True, color='lightgray', zorder=5) \n",
    "\n",
    "# 设置标题\n",
    "# ax.set_title('东京奥运会金牌数-截止8月2日',fontsize=14,y=1.05)\n",
    "# 设置坐标轴标题\n",
    "ax.set_ylabel(\"FPR@1e-3\",fontsize = 12,color = 'black',rotation=90)\n",
    "# 设置Y轴区间\n",
    "# ax.set_ylim(1.3,1.9)\n",
    "ax.set_xticks(x_data)\n",
    "ax.set_xticklabels(x_labels)\n",
    "# # 显示数据标签\n",
    "# for a,b in zip(x_data, y_data):\n",
    "#     plt.text(a,b,\n",
    "#              b,\n",
    "#              ha='center', \n",
    "#              va='bottom',\n",
    "#             )\n",
    "\n",
    "# # Adding the mean values on top of the bars\n",
    "# for bar, mean,std in zip(bar, y_data,std_devs):\n",
    "#     height = bar.get_height()\n",
    "#     ax.text(bar.get_x() + bar.get_width() / 2., height * (1 + std/2+0.01), \n",
    "#             f'{mean:.2f}', ha='center', va='bottom')  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
